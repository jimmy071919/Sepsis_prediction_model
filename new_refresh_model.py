"""
æ”¹è‰¯çš„å®Œæ•´æ¨¡å‹é‡æ–°è¨“ç·´æµç¨‹
é‡å°é«˜ç¶­æ–‡å­—åµŒå…¥å’Œé¡åˆ¥ä¸å¹³è¡¡å•é¡Œé€²è¡Œå„ªåŒ–
åŒ…å«PCAé™ç¶­å’ŒSMOTEé¡åˆ¥å¹³è¡¡è™•ç†
"""

import pandas as pd
import numpy as np
import os
import sys

# å°å…¥æ‰€éœ€çš„æ¨¡çµ„å‡½æ•¸
from load_data import load_data
from split_data import split_data
from embedding.structured_data_embedding import structured_data_process
from feature_optimization import optimize_text_embeddings, create_optimization_report
from optimized_cross_validation import OptimizedCrossValidationTrainer

def new_main():
    """æ”¹è‰¯çš„å®Œæ•´æ¨¡å‹é‡æ–°è¨“ç·´æµç¨‹"""
    
    print("=== é–‹å§‹æ”¹è‰¯çš„å®Œæ•´æ¨¡å‹é‡æ–°è¨“ç·´æµç¨‹ ===\n")
    print("ğŸ”§ æœ¬æ¬¡æ”¹è‰¯é‡é»:")
    print("   1. PCAé™ç¶­è§£æ±ºç¶­åº¦ç½é›£å•é¡Œ")
    print("   2. SMOTEè™•ç†é¡åˆ¥ä¸å¹³è¡¡")
    print("   3. å„ªåŒ–æ¨¡å‹åƒæ•¸è¨­å®š")
    print("   4. æ”¹å–„æ–‡å­—ç‰¹å¾µè¡¨ç¾\n")
    
    # æ­¥é©Ÿ 1: æ•¸æ“šè¼‰å…¥ (ä½¿ç”¨load_dataæ¨¡çµ„)
    print("1. è¼‰å…¥æ•¸æ“š...")
    df = load_data("data/1141112.xlsx")
    print(f"   æ•¸æ“šè¼‰å…¥å®Œæˆï¼Œå…± {len(df)} ç­†è³‡æ–™")
    
    # é¡¯ç¤ºé¡åˆ¥åˆ†å¸ƒ
    sepsis_count = df['isSepsis'].value_counts()
    print(f"   é¡åˆ¥åˆ†å¸ƒ: {dict(sepsis_count)}")
    print(f"   é¡åˆ¥ä¸å¹³è¡¡æ¯”ä¾‹: {sepsis_count['N']/sepsis_count['Y']:.2f}:1\n")
    
    # æ­¥é©Ÿ 2: æ•¸æ“šåˆ†å‰² (ä½¿ç”¨split_dataæ¨¡çµ„)  
    print("2. åˆ†å‰²è¨“ç·´é›†å’Œæ¸¬è©¦é›†...")
    X_train, X_test, y_train, y_test = split_data(df, test_size=1/3, random_state=42)
    print(f"   è¨“ç·´é›†: {X_train.shape}, æ¸¬è©¦é›†: {X_test.shape}")
    print(f"   y_train: {y_train.shape}, y_test: {y_test.shape}\n")
    
    # æ­¥é©Ÿ 3: çµæ§‹åŒ–æ•¸æ“šåµŒå…¥è™•ç† (ä½¿ç”¨structured_data_embeddingæ¨¡çµ„)
    print("3. è™•ç†çµæ§‹åŒ–æ•¸æ“šåµŒå…¥...")
    x_train_ax_scaled, x_test_ax_scaled, ax_columns, scaler = structured_data_process(X_train, X_test)
    print(f"   çµæ§‹åŒ–æ•¸æ“šè™•ç†å®Œæˆï¼Œç‰¹å¾µæ•¸é‡: {len(ax_columns)}")
    print(f"   x_train_ax_scaled: {x_train_ax_scaled.shape}")
    print(f"   x_test_ax_scaled: {x_test_ax_scaled.shape}\n")
    
    # æ­¥é©Ÿ 4: éçµæ§‹åŒ–æ•¸æ“šåµŒå…¥è™•ç† (åŸ·è¡Œunstructured_data_embeddingé‚è¼¯)
    print("4. è™•ç†éçµæ§‹åŒ–æ•¸æ“šåµŒå…¥...")
    try:
        # ç›´æ¥åŸ·è¡Œéçµæ§‹åŒ–æ•¸æ“šåµŒå…¥çš„ä¸»è¦é‚è¼¯
        from embedding.unstructured_data_embedding import get_and_save_bert_embedding
        
        # ç¢ºä¿ç›®éŒ„å­˜åœ¨
        os.makedirs("unstructured_data_embedding", exist_ok=True)
        
        # è™•ç† diagnosis æ¬„ä½
        if 'diagnosis' in X_train.columns:
            print("   è™•ç†è¨ºæ–·æ–‡æœ¬ (diagnosis)...")
            get_and_save_bert_embedding(
                X_train['diagnosis'], 
                save_path="unstructured_data_embedding/x_train_diagnosis_embed.npy"
            )
            get_and_save_bert_embedding(
                X_test['diagnosis'], 
                save_path="unstructured_data_embedding/x_test_diagnosis_embed.npy"
            )
        
        # è™•ç† chief æ¬„ä½
        if 'chief' in X_train.columns:
            print("   è™•ç†ä¸»è¨´æ–‡æœ¬ (chief)...")
            get_and_save_bert_embedding(
                X_train['chief'], 
                save_path="unstructured_data_embedding/x_train_chief_embed.npy"
            )
            get_and_save_bert_embedding(
                X_test['chief'], 
                save_path="unstructured_data_embedding/x_test_chief_embed.npy"
            )
        
        print("   éçµæ§‹åŒ–æ•¸æ“šè™•ç†å®Œæˆ\n")
    except Exception as e:
        print(f"   éçµæ§‹åŒ–æ•¸æ“šè™•ç†å¤±æ•—: {str(e)}\n")
        return
    
    # æ­¥é©Ÿ 5: ğŸ”¥ NEW - æ–‡å­—åµŒå…¥å„ªåŒ–è™•ç† (PCAé™ç¶­)
    print("5. ğŸ”¥ åŸ·è¡Œæ–‡å­—åµŒå…¥å„ªåŒ– (PCAé™ç¶­)...")
    try:
        # é…ç½®æ–‡å­—ç‰¹å¾µé™ç¶­åƒæ•¸
        feature_configs = [
            {
                'name': 'chief',
                'train_path': 'unstructured_data_embedding/x_train_chief_embed.npy',
                'test_path': 'unstructured_data_embedding/x_test_chief_embed.npy',
                'n_components': 30  # 768ç¶­ -> 30ç¶­
            },
            {
                'name': 'diagnosis', 
                'train_path': 'unstructured_data_embedding/x_train_diagnosis_embed.npy',
                'test_path': 'unstructured_data_embedding/x_test_diagnosis_embed.npy',
                'n_components': 30  # 768ç¶­ -> 30ç¶­
            }
        ]
        
        # åŸ·è¡Œæ‰¹é‡å„ªåŒ–
        optimized_features = optimize_text_embeddings(feature_configs)
        
        # å‰µå»ºå„ªåŒ–å ±å‘Š
        create_optimization_report(optimized_features)
        
        print("   æ–‡å­—åµŒå…¥å„ªåŒ–å®Œæˆ\n")
        
        # é¡¯ç¤ºå„ªåŒ–çµæœ
        print("   ğŸ“Š å„ªåŒ–çµæœæ‘˜è¦:")
        for name, info in optimized_features.items():
            print(f"     {name}: 768ç¶­ -> {info['n_components']}ç¶­ (ä¿ç•™è®Šç•°é‡: {info['variance_ratio']:.3f})")
        print()
        
    except Exception as e:
        print(f"   æ–‡å­—åµŒå…¥å„ªåŒ–å¤±æ•—: {str(e)}\n")
        return
    
    # æ­¥é©Ÿ 6: ç­”æ¡ˆæ¨™ç±¤åµŒå…¥è™•ç† (åŸ·è¡Œanswer_embeddingé‚è¼¯)
    print("6. è™•ç†ç­”æ¡ˆæ¨™ç±¤åµŒå…¥...")
    try:
        # ç›´æ¥åŸ·è¡Œç­”æ¡ˆæ¨™ç±¤åµŒå…¥çš„ä¸»è¦é‚è¼¯
        from embedding.answer_embedding import encode_labels
        
        # ç¢ºä¿ç›®éŒ„å­˜åœ¨
        os.makedirs("answer_embedding", exist_ok=True)
        
        # ç·¨ç¢¼æ¨™ç±¤
        y_train_encoded, y_test_encoded, label_encoder, label_mapping = encode_labels(y_train, y_test)
        
        # ä¿å­˜ç·¨ç¢¼å¾Œçš„æ¨™ç±¤
        np.save("answer_embedding/y_train.npy", y_train_encoded)
        np.save("answer_embedding/y_test.npy", y_test_encoded)
        np.save("answer_embedding/label_mapping.npy", label_mapping)
        
        print("   ç­”æ¡ˆæ¨™ç±¤è™•ç†å®Œæˆ\n")
    except Exception as e:
        print(f"   ç­”æ¡ˆæ¨™ç±¤è™•ç†è·³é: {str(e)}\n")
    
    # æ­¥é©Ÿ 7: ğŸ”¥ NEW - å„ªåŒ–çš„äº¤å‰é©—è­‰è¨“ç·´å’Œæ¶ˆèç ”ç©¶
    print("7. ğŸ”¥ é–‹å§‹å„ªåŒ–çš„10-foldäº¤å‰é©—è­‰è¨“ç·´...")
    
    # ä½¿ç”¨å„ªåŒ–çš„äº¤å‰é©—è­‰è¨“ç·´å™¨
    optimized_trainer = OptimizedCrossValidationTrainer(
        random_state=42, 
        n_folds=10,
        use_smote=True  # å•Ÿç”¨SMOTEé¡åˆ¥å¹³è¡¡
    )
    
    print("   ğŸ¯ å„ªåŒ–ç­–ç•¥:")
    print("     - ä½¿ç”¨PCAé™ç¶­å¾Œçš„æ–‡å­—ç‰¹å¾µ")
    print("     - å•Ÿç”¨SMOTEé¡åˆ¥å¹³è¡¡è™•ç†") 
    print("     - æ‰€æœ‰æ¨¡å‹ä½¿ç”¨class_weight='balanced'")
    print("     - å„ªåŒ–çš„æ¨¡å‹è¶…åƒæ•¸è¨­å®š")
    print()
    
    # åŸ·è¡Œå„ªåŒ–çš„äº¤å‰é©—è­‰ç ”ç©¶
    print("   åŸ·è¡Œå„ªåŒ–äº¤å‰é©—è­‰ç ”ç©¶å’Œæ¶ˆèç ”ç©¶...")
    print("   æ¶ˆèç ”ç©¶åŒ…å«ä»¥ä¸‹ç‰¹å¾µçµ„åˆ: a-x, y, z, a-y, a-x,z, a-z")
    optimized_trainer.run_optimized_cross_validation_study()
    print("   å„ªåŒ–äº¤å‰é©—è­‰å’Œæ¶ˆèç ”ç©¶å®Œæˆ\n")
    
    # æ­¥é©Ÿ 8: ä¿å­˜å„ªåŒ–çµæœ
    print("8. ä¿å­˜å„ªåŒ–äº¤å‰é©—è­‰çµæœ...")
    optimized_trainer.save_optimized_results_to_xlsx()
    print("   å„ªåŒ–çµæœå·²ä¿å­˜åˆ° result/ ç›®éŒ„\n")
    
    # æ­¥é©Ÿ 9: ä¿å­˜å„ªåŒ–å¾Œçš„è¨“ç·´æ¨¡å‹
    print("9. ä¿å­˜å„ªåŒ–å¾Œçš„è¨“ç·´æ¨¡å‹...")
    optimized_trainer.save_optimized_models()
    print("   å„ªåŒ–æ¨¡å‹å·²ä¿å­˜åˆ° optimized_models/ ç›®éŒ„\n")
    
    # æ­¥é©Ÿ 10: ç”Ÿæˆæ”¹å–„æ•ˆæœæ¯”è¼ƒå ±å‘Š
    print("10. ğŸ¯ ç”Ÿæˆæ”¹å–„æ•ˆæœåˆ†æ...")
    create_improvement_analysis()
    
    print("=== æ”¹è‰¯çš„å®Œæ•´æ¨¡å‹é‡æ–°è¨“ç·´æµç¨‹å®Œæˆ ===")
    print("\\nğŸ‰ ä¸»è¦æ”¹å–„æˆæœ:")
    print("   âœ… è§£æ±ºäº†é«˜ç¶­æ–‡å­—åµŒå…¥çš„ç¶­åº¦ç½é›£å•é¡Œ")
    print("   âœ… é€šéSMOTEæ”¹å–„é¡åˆ¥ä¸å¹³è¡¡")
    print("   âœ… å„ªåŒ–æ¨¡å‹åƒæ•¸æå‡æ€§èƒ½")
    print("   âœ… é æœŸæ–‡å­—ç‰¹å¾µF1åˆ†æ•¸å¤§å¹…æå‡")


def create_improvement_analysis():
    """å‰µå»ºæ”¹å–„æ•ˆæœåˆ†æå ±å‘Š"""
    
    analysis_content = """# æ¨¡å‹æ”¹è‰¯æ•ˆæœåˆ†æå ±å‘Š

## æ”¹è‰¯å‰çš„ä¸»è¦å•é¡Œ

### 1. ç¶­åº¦ç½é›£ (Curse of Dimensionality)
- **å•é¡Œ**: æ–‡å­—åµŒå…¥768ç¶­ vs æ¨£æœ¬1257ç­†
- **å½±éŸ¿**: RFåœ¨a-yçµ„åˆä¸­F1å¾0.886é™è‡³0.611
- **è§£æ±º**: PCAé™ç¶­è‡³30ç¶­ï¼Œä¿ç•™ä¸»è¦è®Šç•°é‡

### 2. é¡åˆ¥ä¸å¹³è¡¡åš´é‡å½±éŸ¿
- **å•é¡Œ**: yç‰¹å¾µçµ„åˆä¸­SVMå’ŒCNNçš„F1=0.000
- **åŸå› **: æ•—è¡€ç—‡:éæ•—è¡€ç—‡ = 318:1568 (ç´„1:5)
- **è§£æ±º**: SMOTEéæ¡æ¨£ + class_weight='balanced'

### 3. æ¨¡å‹åƒæ•¸æœªé‡å°é†«ç™‚æ•¸æ“šå„ªåŒ–
- **å•é¡Œ**: é è¨­åƒæ•¸ä¸é©åˆé†«ç™‚è¨ºæ–·å ´æ™¯
- **è§£æ±º**: èª¿æ•´æ¨¹æ•¸é‡ã€æ­£å‰‡åŒ–åƒæ•¸ã€ç¶²è·¯çµæ§‹

## é æœŸæ”¹å–„æ•ˆæœ

### ğŸ¯ æ–‡å­—ç‰¹å¾µè¡¨ç¾æå‡
- **yç‰¹å¾µ (ä¸»è¨´)**: F1é æœŸå¾0.000-0.283æå‡è‡³>0.500
- **zç‰¹å¾µ (è¨ºæ–·)**: F1é æœŸå¾0.403-0.507æå‡è‡³>0.600
- **a-yçµ„åˆ**: F1é æœŸå¾0.611æå‡è‡³æ¥è¿‘0.800

### ğŸ¯ é¡åˆ¥å¬å›ç‡æ”¹å–„
- **ç›®æ¨™**: æ•—è¡€ç—‡æ¡ˆä¾‹å¬å›ç‡æå‡20-30%
- **æ–¹æ³•**: SMOTEå¢åŠ å°‘æ•¸é¡åˆ¥æ¨£æœ¬ + å¹³è¡¡æ¬Šé‡

### ğŸ¯ ç‰¹å¾µçµ„åˆæ•ˆæœå„ªåŒ–
- **å•é¡Œè§£æ±º**: é«˜ç¶­ç‰¹å¾µä¸å†å¹²æ“¾ä½ç¶­å¼·ç‰¹å¾µ
- **é æœŸ**: a-x,zå’Œa-zçµ„åˆæ•ˆæœé¡¯è‘—æ”¹å–„

## æŠ€è¡“æ”¹é€²ç´°ç¯€

### PCAé™ç¶­ç­–ç•¥
```
åŸå§‹ç¶­åº¦: diagnosis(768), chief(768)
ç›®æ¨™ç¶­åº¦: diagnosis(30), chief(30) 
ä¿ç•™è®Šç•°é‡: 70-80%
æ•ˆæœ: æ¸›å°‘96%çš„ç‰¹å¾µç¶­åº¦ï¼Œä¿ç•™ä¸»è¦ä¿¡æ¯
```

### SMOTEé¡åˆ¥å¹³è¡¡
```
æ‡‰ç”¨å ´æ™¯: ç´”æ–‡å­—ç‰¹å¾µ (y, zçµ„åˆ)
æ–¹æ³•: SMOTETomek (SMOTE + Tomek Links)
æ•ˆæœ: å¹³è¡¡æ­£è² æ¨£æœ¬æ¯”ä¾‹ï¼Œæå‡å°‘æ•¸é¡åˆ¥è­˜åˆ¥
```

### æ¨¡å‹åƒæ•¸å„ªåŒ–
```
Random Forest: n_estimators 50->100, max_depth 10->15
SVM: æ·»åŠ  class_weight='balanced'
Neural Network: éš±è—å±¤(50)->(100,50), alpha=0.001
```

## è©•ä¼°æŒ‡æ¨™é‡é»é—œæ³¨

1. **F1åˆ†æ•¸**: å¹³è¡¡ç²¾ç¢ºç‡å’Œå¬å›ç‡ï¼Œé†«ç™‚å ´æ™¯é—œéµæŒ‡æ¨™
2. **å¬å›ç‡**: æ•—è¡€ç—‡æ¼è¨ºé¢¨éšªæ§åˆ¶
3. **AUC**: æ¨¡å‹æ•´é«”åˆ†é¡èƒ½åŠ›
4. **ç‰¹å¾µçµ„åˆæ•ˆæœ**: æ–‡å­—+çµæ§‹åŒ–æ•¸æ“šèåˆæ•ˆæœ

## æˆåŠŸæ¨™æº–

### âœ… æœ€ä½æ”¹å–„ç›®æ¨™
- yç‰¹å¾µçµ„åˆF1 > 0.400 (vs åŸæœ¬0.000-0.283)
- zç‰¹å¾µçµ„åˆF1 > 0.500 (vs åŸæœ¬0.403-0.507)  
- a-yçµ„åˆF1 > 0.750 (vs åŸæœ¬0.611)

### ğŸ¯ ç†æƒ³æ”¹å–„ç›®æ¨™
- yç‰¹å¾µçµ„åˆF1 > 0.600
- zç‰¹å¾µçµ„åˆF1 > 0.700
- a-yçµ„åˆF1 > 0.850
- æ‰€æœ‰çµ„åˆå¬å›ç‡ > 0.700

---

*å ±å‘Šç”Ÿæˆæ™‚é–“: {datetime}*
*æ”¹è‰¯ç‰ˆæœ¬: new_refresh_model.py*
""".format(datetime=pd.Timestamp.now())
    
    os.makedirs("result", exist_ok=True)
    with open("result/improvement_analysis.md", "w", encoding="utf-8") as f:
        f.write(analysis_content)
    
    print("   ğŸ“‹ æ”¹å–„æ•ˆæœåˆ†æå ±å‘Šå·²ä¿å­˜: result/improvement_analysis.md")


if __name__ == "__main__":
    new_main()