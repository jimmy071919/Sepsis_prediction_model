"""
æ”¹è‰¯çš„äº¤å‰é©—è­‰è¨“ç·´å™¨
é‡å°é¡åˆ¥ä¸å¹³è¡¡å’Œé«˜ç¶­ç‰¹å¾µå„ªåŒ–çš„ç‰ˆæœ¬
"""

import pandas as pd
import numpy as np
from sklearn.model_selection import StratifiedKFold, cross_validate
from sklearn.tree import DecisionTreeClassifier
from sklearn.svm import SVC
from sklearn.ensemble import RandomForestClassifier
from sklearn.neural_network import MLPClassifier
from sklearn.metrics import make_scorer, precision_score, recall_score, f1_score, roc_auc_score
from sklearn.impute import SimpleImputer
from imblearn.over_sampling import SMOTE
from imblearn.combine import SMOTETomek
import joblib
import os
import warnings
warnings.filterwarnings('ignore')

class OptimizedCrossValidationTrainer:
    """å„ªåŒ–çš„äº¤å‰é©—è­‰è¨“ç·´å™¨"""
    
    def __init__(self, random_state=42, n_folds=10, use_smote=True):
        """
        åˆå§‹åŒ–å„ªåŒ–çš„äº¤å‰é©—è­‰è¨“ç·´å™¨
        
        Args:
            random_state: éš¨æ©Ÿç¨®å­
            n_folds: äº¤å‰é©—è­‰æŠ˜æ•¸
            use_smote: æ˜¯å¦ä½¿ç”¨SMOTEè™•ç†é¡åˆ¥ä¸å¹³è¡¡
        """
        self.random_state = random_state
        self.n_folds = n_folds
        self.use_smote = use_smote
        self.models = self._initialize_optimized_models()
        self.cv_results = {}
        self.test_results = {}
        self.trained_models = {}
        # åˆå§‹åŒ–æ•¸æ“šå¡«å……å™¨
        self.imputer = SimpleImputer(strategy='median')
        # åˆå§‹åŒ–äº¤å‰é©—è­‰ç­–ç•¥
        self.cv_strategy = StratifiedKFold(n_splits=n_folds, shuffle=True, random_state=random_state)
        # SMOTEåˆå§‹åŒ–
        if self.use_smote:
            self.smote = SMOTETomek(random_state=random_state)
        
    def _initialize_optimized_models(self):
        """åˆå§‹åŒ–å„ªåŒ–çš„æ¨¡å‹ï¼ˆé‡å°é¡åˆ¥ä¸å¹³è¡¡èª¿æ•´åƒæ•¸ï¼‰"""
        models = {
            'DT': DecisionTreeClassifier(
                random_state=self.random_state, 
                max_depth=10,
                class_weight='balanced'  # è™•ç†é¡åˆ¥ä¸å¹³è¡¡
            ),
            'SVM': SVC(
                random_state=self.random_state, 
                probability=True, 
                kernel='rbf', 
                C=1.0,
                class_weight='balanced'  # è™•ç†é¡åˆ¥ä¸å¹³è¡¡
            ),
            'RF': RandomForestClassifier(
                random_state=self.random_state, 
                n_estimators=100,  # å¢åŠ æ¨¹çš„æ•¸é‡
                max_depth=15,  # ç¨å¾®å¢åŠ æ·±åº¦
                class_weight='balanced'  # è™•ç†é¡åˆ¥ä¸å¹³è¡¡
            ),
            'CNN': MLPClassifier(
                random_state=self.random_state, 
                hidden_layer_sizes=(100, 50),  # èª¿æ•´ç¶²è·¯çµæ§‹
                max_iter=500,  # å¢åŠ è¿­ä»£æ¬¡æ•¸
                early_stopping=True, 
                validation_fraction=0.1,
                alpha=0.001  # æ­£å‰‡åŒ–åƒæ•¸
            )
        }
        return models
    
    def _get_scoring_metrics(self):
        """å®šç¾©è©•ä¼°æŒ‡æ¨™"""
        scoring = {
            'auc': 'roc_auc',
            'precision': make_scorer(precision_score, average='binary', zero_division=0),
            'recall': make_scorer(recall_score, average='binary', zero_division=0),
            'f1': make_scorer(f1_score, average='binary', zero_division=0)
        }
        return scoring
    
    def load_and_prepare_optimized_data(self, feature_type):
        """è¼‰å…¥å’Œæº–å‚™å„ªåŒ–å¾Œçš„æ•¸æ“š"""
        try:
            # è¼‰å…¥çµæ§‹åŒ–æ•¸æ“š
            X_train_structured = np.load('structured_data_embedding/x_train_ax_scaled.npy')
            X_test_structured = np.load('structured_data_embedding/x_test_ax_scaled.npy')
            
            # è¼‰å…¥å„ªåŒ–å¾Œçš„æ–‡æœ¬åµŒå…¥ï¼ˆPCAé™ç¶­å¾Œï¼‰
            X_train_chief = np.load('optimized_embeddings/x_train_chief_pca.npy')
            X_test_chief = np.load('optimized_embeddings/x_test_chief_pca.npy')
            
            X_train_diagnosis = np.load('optimized_embeddings/x_train_diagnosis_pca.npy')
            X_test_diagnosis = np.load('optimized_embeddings/x_test_diagnosis_pca.npy')
            
            # è¼‰å…¥æ¨™ç±¤
            y_train = np.load('answer_embedding/y_train.npy')
            y_test = np.load('answer_embedding/y_test.npy')
            
        except FileNotFoundError as e:
            raise FileNotFoundError(f"å„ªåŒ–å¾Œçš„æ•¸æ“šæ–‡ä»¶ä¸å­˜åœ¨: {e}")
        
        # æ ¹æ“šç‰¹å¾µé¡å‹çµ„åˆæ•¸æ“š
        if feature_type == 'a-x':
            X_train = X_train_structured
            X_test = X_test_structured
        elif feature_type == 'y':
            X_train = X_train_chief
            X_test = X_test_chief
        elif feature_type == 'z':
            X_train = X_train_diagnosis
            X_test = X_test_diagnosis
        elif feature_type == 'a-y':
            X_train = np.concatenate([X_train_structured, X_train_chief], axis=1)
            X_test = np.concatenate([X_test_structured, X_test_chief], axis=1)
        elif feature_type == 'a-x,z':
            X_train = np.concatenate([X_train_structured, X_train_diagnosis], axis=1)
            X_test = np.concatenate([X_test_structured, X_test_diagnosis], axis=1)
        elif feature_type == 'a-z':
            X_train = np.concatenate([X_train_structured, X_train_chief, X_train_diagnosis], axis=1)
            X_test = np.concatenate([X_test_structured, X_test_chief, X_test_diagnosis], axis=1)
        else:
            raise ValueError(f"ä¸æ”¯æ´çš„ç‰¹å¾µé¡å‹: {feature_type}")
        
        print(f"   ç‰¹å¾µé¡å‹ {feature_type} - è¨“ç·´é›†: {X_train.shape}, æ¸¬è©¦é›†: {X_test.shape}")
        
        return X_train, X_test, y_train, y_test
    
    def _optimized_cross_validate(self, model, X, y, feature_type):
        """å„ªåŒ–çš„äº¤å‰é©—è­‰ï¼ŒåŒ…å«SMOTEè™•ç†"""
        from sklearn.base import clone
        
        cv_scores = {'auc': [], 'precision': [], 'recall': [], 'f1': []}
        
        for fold_idx, (train_idx, val_idx) in enumerate(self.cv_strategy.split(X, y)):
            print(f"    è™•ç† Fold {fold_idx + 1}/{self.n_folds}...")
            
            # åˆ†å‰²ç•¶å‰foldçš„æ•¸æ“š
            X_fold_train, X_fold_val = X[train_idx], X[val_idx]
            y_fold_train, y_fold_val = y[train_idx], y[val_idx]
            
            # è™•ç†NaNå€¼ï¼ˆå¦‚æœåŒ…å«çµæ§‹åŒ–æ•¸æ“šï¼‰
            if feature_type in ['a-x', 'a-y', 'a-x,z', 'a-z'] and np.isnan(X_fold_train).any():
                fold_imputer = SimpleImputer(strategy='median')
                X_fold_train = fold_imputer.fit_transform(X_fold_train)
                X_fold_val = fold_imputer.transform(X_fold_val)
            
            # å°ç´”æ–‡æœ¬ç‰¹å¾µä½¿ç”¨SMOTE
            if self.use_smote and feature_type in ['y', 'z']:
                try:
                    X_fold_train, y_fold_train = self.smote.fit_resample(X_fold_train, y_fold_train)
                    print(f"      Fold {fold_idx + 1} SMOTEå¾Œæ•¸é‡: {len(y_fold_train)}")
                except Exception as e:
                    print(f"      Fold {fold_idx + 1} SMOTEå¤±æ•—: {e}")
            
            # è¨“ç·´æ¨¡å‹
            fold_model = clone(model)
            fold_model.fit(X_fold_train, y_fold_train)
            
            # é æ¸¬
            y_pred = fold_model.predict(X_fold_val)
            y_prob = fold_model.predict_proba(X_fold_val)[:, 1] if hasattr(fold_model, 'predict_proba') else fold_model.decision_function(X_fold_val)
            
            # è¨ˆç®—æŒ‡æ¨™
            cv_scores['auc'].append(roc_auc_score(y_fold_val, y_prob))
            cv_scores['precision'].append(precision_score(y_fold_val, y_pred, average='binary', zero_division=0))
            cv_scores['recall'].append(recall_score(y_fold_val, y_pred, average='binary', zero_division=0))
            cv_scores['f1'].append(f1_score(y_fold_val, y_pred, average='binary', zero_division=0))
        
        # è½‰æ›ç‚ºnumpyæ•¸çµ„
        for metric in cv_scores:
            cv_scores[metric] = np.array(cv_scores[metric])
        
        return cv_scores
    
    def perform_optimized_cross_validation(self, feature_type):
        """åŸ·è¡Œå„ªåŒ–çš„äº¤å‰é©—è­‰"""
        print(f"\\n=== é–‹å§‹å„ªåŒ–çš„ {feature_type} ç‰¹å¾µäº¤å‰é©—è­‰ ===")
        
        # è¼‰å…¥å„ªåŒ–å¾Œçš„æ•¸æ“š
        X_train, X_test, y_train, y_test = self.load_and_prepare_optimized_data(feature_type)
        
        cv_results = {}
        test_results = {}
        
        # å°æ¯å€‹æ¨¡å‹é€²è¡Œäº¤å‰é©—è­‰
        for model_name, model in self.models.items():
            print(f"\\næ­£åœ¨é€²è¡Œ {model_name} æ¨¡å‹çš„å„ªåŒ–äº¤å‰é©—è­‰...")
            
            try:
                # åŸ·è¡Œå„ªåŒ–çš„äº¤å‰é©—è­‰
                cv_scores = self._optimized_cross_validate(model, X_train, y_train, feature_type)
                
                # è¨ˆç®—äº¤å‰é©—è­‰çµæœ
                cv_result = {
                    'AUC_mean': np.mean(cv_scores['auc']),
                    'AUC_std': np.std(cv_scores['auc']),
                    'precision_mean': np.mean(cv_scores['precision']),
                    'precision_std': np.std(cv_scores['precision']),
                    'recall_mean': np.mean(cv_scores['recall']),
                    'recall_std': np.std(cv_scores['recall']),
                    'f1_mean': np.mean(cv_scores['f1']),
                    'f1_std': np.std(cv_scores['f1'])
                }
                
                cv_results[model_name] = cv_result
                
                print(f"{model_name} å„ªåŒ–äº¤å‰é©—è­‰å®Œæˆ:")
                print(f"  AUC: {cv_result['AUC_mean']:.3f} (Â±{cv_result['AUC_std']:.3f})")
                print(f"  F1: {cv_result['f1_mean']:.3f} (Â±{cv_result['f1_std']:.3f})")
                print(f"  Recall: {cv_result['recall_mean']:.3f} (Â±{cv_result['recall_std']:.3f})")
                
                # åœ¨æ¸¬è©¦é›†ä¸Šè©•ä¼°
                print(f"æ­£åœ¨è¨“ç·´ {model_name} ä¸¦åœ¨æ¸¬è©¦é›†ä¸Šè©•ä¼°...")
                
                # æº–å‚™æœ€çµ‚è¨“ç·´æ•¸æ“š
                X_train_final = X_train.copy()
                X_test_final = X_test.copy()
                y_train_final = y_train.copy()
                
                # è™•ç†NaNå€¼
                if feature_type in ['a-x', 'a-y', 'a-x,z', 'a-z'] and np.isnan(X_train_final).any():
                    final_imputer = SimpleImputer(strategy='median')
                    X_train_final = final_imputer.fit_transform(X_train_final)
                    X_test_final = final_imputer.transform(X_test_final)
                
                # å°ç´”æ–‡æœ¬ç‰¹å¾µä½¿ç”¨SMOTE
                if self.use_smote and feature_type in ['y', 'z']:
                    try:
                        X_train_final, y_train_final = self.smote.fit_resample(X_train_final, y_train_final)
                        print(f"  æœ€çµ‚è¨“ç·´é›†SMOTEå¾Œæ•¸é‡: {len(y_train_final)}")
                    except Exception as e:
                        print(f"  æœ€çµ‚è¨“ç·´é›†SMOTEå¤±æ•—: {e}")
                
                # è¨“ç·´æœ€çµ‚æ¨¡å‹
                model.fit(X_train_final, y_train_final)
                
                # ä¿å­˜æ¨¡å‹
                if feature_type not in self.trained_models:
                    self.trained_models[feature_type] = {}
                self.trained_models[feature_type][model_name] = model
                
                # æ¸¬è©¦é›†é æ¸¬
                y_test_pred = model.predict(X_test_final)
                y_test_prob = model.predict_proba(X_test_final)[:, 1] if hasattr(model, 'predict_proba') else model.decision_function(X_test_final)
                
                # è¨ˆç®—æ¸¬è©¦é›†æŒ‡æ¨™
                test_result = {
                    'AUC': roc_auc_score(y_test, y_test_prob),
                    'precision': precision_score(y_test, y_test_pred, average='binary', zero_division=0),
                    'recall': recall_score(y_test, y_test_pred, average='binary', zero_division=0),
                    'f1': f1_score(y_test, y_test_pred, average='binary', zero_division=0)
                }
                
                test_results[model_name] = test_result
                
                print(f"{model_name} æ¸¬è©¦é›†çµæœ:")
                print(f"  AUC: {test_result['AUC']:.3f}")
                print(f"  F1: {test_result['f1']:.3f}")
                print(f"  Recall: {test_result['recall']:.3f}")
                
            except Exception as e:
                print(f"{model_name} è¨“ç·´å¤±æ•—: {str(e)}")
                # å¡«å……ç©ºçµæœ
                empty_cv_result = {
                    'AUC_mean': 0, 'AUC_std': 0,
                    'precision_mean': 0, 'precision_std': 0,
                    'recall_mean': 0, 'recall_std': 0,
                    'f1_mean': 0, 'f1_std': 0
                }
                empty_test_result = {'AUC': 0, 'precision': 0, 'recall': 0, 'f1': 0}
                cv_results[model_name] = empty_cv_result
                test_results[model_name] = empty_test_result
        
        # ä¿å­˜çµæœ
        self.cv_results[feature_type] = cv_results
        self.test_results[feature_type] = test_results
        
        return cv_results, test_results
    
    def run_optimized_cross_validation_study(self):
        """åŸ·è¡Œå®Œæ•´çš„å„ªåŒ–äº¤å‰é©—è­‰ç ”ç©¶"""
        feature_types = ['a-x', 'y', 'z', 'a-y', 'a-x,z', 'a-z']
        
        print(f"=== é–‹å§‹å„ªåŒ–çš„ {self.n_folds}-fold äº¤å‰é©—è­‰ç ”ç©¶ ===")
        if self.use_smote:
            print("ğŸ“ˆ å•Ÿç”¨SMOTEé¡åˆ¥å¹³è¡¡è™•ç†")
        
        for feature_type in feature_types:
            try:
                self.perform_optimized_cross_validation(feature_type)
            except Exception as e:
                print(f"ç‰¹å¾µé¡å‹ {feature_type} å„ªåŒ–äº¤å‰é©—è­‰å¤±æ•—: {str(e)}")
                continue
        
        # ç”Ÿæˆæ¯”è¼ƒå ±å‘Š
        self.create_optimization_comparison()
    
    def create_optimization_comparison(self):
        """å‰µå»ºå„ªåŒ–å‰å¾Œçš„æ¯”è¼ƒå ±å‘Š"""
        print(f"\\n\\n=== å„ªåŒ–äº¤å‰é©—è­‰çµæœç¸½çµ ===")
        
        for feature_type in self.cv_results:
            print(f"\\n=== {feature_type} ç‰¹å¾µï¼ˆå„ªåŒ–ç‰ˆæœ¬ï¼‰ ===")
            
            # äº¤å‰é©—è­‰çµæœ
            print(f"\\n{self.n_folds}-fold å„ªåŒ–äº¤å‰é©—è­‰çµæœ:")
            cv_df_data = []
            for model_name, results in self.cv_results[feature_type].items():
                cv_df_data.append({
                    'Model': model_name,
                    'AUC': f"{results['AUC_mean']:.3f} Â± {results['AUC_std']:.3f}",
                    'Precision': f"{results['precision_mean']:.3f} Â± {results['precision_std']:.3f}",
                    'Recall': f"{results['recall_mean']:.3f} Â± {results['recall_std']:.3f}",
                    'F1': f"{results['f1_mean']:.3f} Â± {results['f1_std']:.3f}"
                })
            
            cv_df = pd.DataFrame(cv_df_data)
            print(cv_df.to_string(index=False))
            
            # æ¸¬è©¦é›†çµæœ
            print(f"\\næ¸¬è©¦é›†è©•ä¼°çµæœ:")
            test_df_data = []
            for model_name, results in self.test_results[feature_type].items():
                test_df_data.append({
                    'Model': model_name,
                    'AUC': f"{results['AUC']:.3f}",
                    'Precision': f"{results['precision']:.3f}",
                    'Recall': f"{results['recall']:.3f}",
                    'F1': f"{results['f1']:.3f}"
                })
            
            test_df = pd.DataFrame(test_df_data)
            print(test_df.to_string(index=False))
    
    def save_optimized_results_to_xlsx(self):
        """ä¿å­˜å„ªåŒ–çµæœåˆ°Excel"""
        import openpyxl
        from openpyxl.styles import Font, PatternFill, Alignment, Border, Side
        from datetime import datetime
        
        excel_filename = 'result/optimized_cross_validation_results.xlsx'
        print(f"\\nğŸ“Š æ­£åœ¨ä¿å­˜å„ªåŒ–çµæœåˆ°: {excel_filename}")
        
        wb = openpyxl.Workbook()
        wb.remove(wb.active)
        
        # å®šç¾©æ¨£å¼
        header_font = Font(bold=True, color="FFFFFF")
        header_fill = PatternFill(start_color="0066CC", end_color="0066CC", fill_type="solid")
        border = Border(
            left=Side(border_style="thin"),
            right=Side(border_style="thin"),
            top=Side(border_style="thin"),
            bottom=Side(border_style="thin")
        )
        center_alignment = Alignment(horizontal="center", vertical="center")
        
        # å‰µå»ºæ‘˜è¦å·¥ä½œè¡¨
        summary_ws = wb.create_sheet("ğŸ“Š å„ªåŒ–çµæœæ‘˜è¦")
        summary_ws['A1'] = f"æ•—è¡€ç—‡é æ¸¬æ¨¡å‹å„ªåŒ–ç‰ˆ {self.n_folds}-fold äº¤å‰é©—è­‰çµæœ"
        summary_ws['A1'].font = Font(size=16, bold=True)
        summary_ws['A2'] = f"ç”Ÿæˆæ™‚é–“: {datetime.now().strftime('%Yå¹´%mæœˆ%dæ—¥ %H:%M:%S')}"
        summary_ws['A3'] = f"å„ªåŒ–æªæ–½: PCAé™ç¶­ + é¡åˆ¥å¹³è¡¡è™•ç†"
        
        # ç‚ºæ¯å€‹ç‰¹å¾µé¡å‹å‰µå»ºå·¥ä½œè¡¨
        for feature_type in self.cv_results:
            self._create_optimized_feature_sheet(wb, feature_type, header_font, header_fill, border, center_alignment)
        
        wb.save(excel_filename)
        print(f"âœ… å„ªåŒ–çµæœå·²æˆåŠŸä¿å­˜åˆ° {excel_filename}")
    
    def _create_optimized_feature_sheet(self, wb, feature_type, header_font, header_fill, border, center_alignment):
        """ç‚ºç‰¹å®šç‰¹å¾µé¡å‹å‰µå»ºå·¥ä½œè¡¨"""
        from openpyxl.styles import Font
        
        ws = wb.create_sheet(f"{feature_type}_å„ªåŒ–ç‰ˆ")
        
        # äº¤å‰é©—è­‰çµæœ
        ws['A1'] = f"{feature_type} ç‰¹å¾µçµ„åˆ - å„ªåŒ–äº¤å‰é©—è­‰çµæœ"
        ws['A1'].font = Font(size=14, bold=True)
        
        cv_headers = ['Model', 'AUC_Mean', 'AUC_Std', 'Precision_Mean', 'Precision_Std', 
                     'Recall_Mean', 'Recall_Std', 'F1_Mean', 'F1_Std']
        
        for col, header in enumerate(cv_headers, 1):
            cell = ws.cell(row=3, column=col, value=header)
            cell.font = header_font
            cell.fill = header_fill
            cell.alignment = center_alignment
            cell.border = border
        
        # å¡«å……äº¤å‰é©—è­‰æ•¸æ“š
        row = 4
        for model_name, results in self.cv_results[feature_type].items():
            ws.cell(row=row, column=1, value=model_name)
            ws.cell(row=row, column=2, value=results['AUC_mean'])
            ws.cell(row=row, column=3, value=results['AUC_std'])
            ws.cell(row=row, column=4, value=results['precision_mean'])
            ws.cell(row=row, column=5, value=results['precision_std'])
            ws.cell(row=row, column=6, value=results['recall_mean'])
            ws.cell(row=row, column=7, value=results['recall_std'])
            ws.cell(row=row, column=8, value=results['f1_mean'])
            ws.cell(row=row, column=9, value=results['f1_std'])
            row += 1
        
        # æ¸¬è©¦é›†çµæœ  
        from openpyxl.styles import Font
        ws.cell(row=row+1, column=1, value="æ¸¬è©¦é›†çµæœ").font = Font(size=12, bold=True)
        test_headers = ['Model', 'AUC', 'Precision', 'Recall', 'F1']
        
        for col, header in enumerate(test_headers, 1):
            cell = ws.cell(row=row+3, column=col, value=header)
            cell.font = header_font
            cell.fill = header_fill
            cell.alignment = center_alignment
            cell.border = border
        
        # å¡«å……æ¸¬è©¦é›†æ•¸æ“š
        test_row = row + 4
        for model_name, results in self.test_results[feature_type].items():
            ws.cell(row=test_row, column=1, value=model_name)
            ws.cell(row=test_row, column=2, value=results['AUC'])
            ws.cell(row=test_row, column=3, value=results['precision'])
            ws.cell(row=test_row, column=4, value=results['recall'])
            ws.cell(row=test_row, column=5, value=results['f1'])
            test_row += 1
    
    def save_optimized_models(self):
        """ä¿å­˜å„ªåŒ–å¾Œçš„æ¨¡å‹"""
        if not os.path.exists('optimized_models'):
            os.makedirs('optimized_models')
        
        print(f"\\nğŸ¤– æ­£åœ¨ä¿å­˜å„ªåŒ–å¾Œçš„æ¨¡å‹...")
        
        saved_count = 0
        for feature_type in self.trained_models:
            for model_name, model in self.trained_models[feature_type].items():
                model_filename = f'optimized_models/{feature_type.replace("-", "_").replace(",", "_")}_{model_name}_optimized.pkl'
                joblib.dump(model, model_filename)
                saved_count += 1
                print(f"   âœ… {feature_type}-{model_name} å„ªåŒ–æ¨¡å‹å·²ä¿å­˜: {model_filename}")
        
        print(f"\\nğŸ“ ç¸½å…±ä¿å­˜äº† {saved_count} å€‹å„ªåŒ–æ¨¡å‹åˆ° optimized_models/ ç›®éŒ„")
        
        # å‰µå»ºå„ªåŒ–æ¨¡å‹ä½¿ç”¨èªªæ˜
        self._create_optimized_model_guide()
    
    def _create_optimized_model_guide(self):
        """å‰µå»ºå„ªåŒ–æ¨¡å‹ä½¿ç”¨èªªæ˜"""
        guide_content = '''# å„ªåŒ–æ¨¡å‹ä½¿ç”¨èªªæ˜

## å„ªåŒ–å…§å®¹

### 1. PCAé™ç¶­
- è¨ºæ–·æ–‡æœ¬åµŒå…¥: 768ç¶­ â†’ 30ç¶­
- ä¸»è¨´æ–‡æœ¬åµŒå…¥: 768ç¶­ â†’ 30ç¶­
- ä¿ç•™ç´„70-80%çš„è®Šç•°é‡

### 2. é¡åˆ¥å¹³è¡¡è™•ç†
- å°æ–‡æœ¬ç‰¹å¾µä½¿ç”¨SMOTEéæ¡æ¨£
- æ‰€æœ‰æ¨¡å‹ä½¿ç”¨class_weight='balanced'
- é‡å°é¡åˆ¥ä¸å¹³è¡¡å•é¡Œå„ªåŒ–

### 3. æ¨¡å‹åƒæ•¸èª¿å„ª
- Random Forest: å¢åŠ æ¨¹æ•¸é‡ï¼Œèª¿æ•´æ·±åº¦
- SVM: æ·»åŠ é¡åˆ¥æ¬Šé‡å¹³è¡¡
- Neural Network: èª¿æ•´ç¶²è·¯çµæ§‹å’Œæ­£å‰‡åŒ–

## é æœŸæ”¹å–„æ•ˆæœ

1. **æ–‡æœ¬ç‰¹å¾µF1åˆ†æ•¸æå‡**: ç‰¹åˆ¥æ˜¯yå’Œzç‰¹å¾µçµ„åˆ
2. **é™ä½éæ“¬åˆ**: é€šéPCAé™ç¶­æ¸›å°‘ç¶­åº¦ç½é›£
3. **æ”¹å–„å¬å›ç‡**: é€šéSMOTEå’Œclass_weightè™•ç†é¡åˆ¥ä¸å¹³è¡¡
4. **æé«˜è¨“ç·´æ•ˆç‡**: è¼ƒä½ç¶­åº¦çš„ç‰¹å¾µç©ºé–“

## ä½¿ç”¨æ–¹å¼

```python
import joblib
import numpy as np

# è¼‰å…¥å„ªåŒ–æ¨¡å‹
model = joblib.load('optimized_models/a_y_RF_optimized.pkl')

# æ³¨æ„ï¼šè¼¸å…¥æ•¸æ“šéœ€è¦ç¶“éç›¸åŒçš„é è™•ç†
# 1. çµæ§‹åŒ–æ•¸æ“šéœ€è¦æ¨™æº–åŒ–
# 2. æ–‡æœ¬æ•¸æ“šéœ€è¦PCAé™ç¶­åˆ°ç›¸åŒç¶­åº¦
```

## é‡è¦æé†’

ä½¿ç”¨å„ªåŒ–æ¨¡å‹å‰ï¼Œç¢ºä¿ï¼š
1. æ–°æ•¸æ“šç¶“éç›¸åŒçš„PCAè½‰æ›
2. çµæ§‹åŒ–æ•¸æ“šä½¿ç”¨ç›¸åŒçš„æ¨™æº–åŒ–å™¨
3. ç‰¹å¾µé †åºèˆ‡è¨“ç·´æ™‚ä¸€è‡´
'''
        
        with open('optimized_models/README.md', 'w', encoding='utf-8') as f:
            f.write(guide_content)
        
        print("ğŸ“ å„ªåŒ–æ¨¡å‹ä½¿ç”¨èªªæ˜å·²ä¿å­˜: optimized_models/README.md")